{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n7mNJhSWwSZ2",
        "outputId": "8e5ac939-845d-4a2e-b878-6dd3c54dc3ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1gzUTR5H-qEo"
      },
      "source": [
        "**Importing lib**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gN9P29lzAsz3",
        "outputId": "40cc4a0a-4822-49a0-ab51-00fdc7fe980a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import string\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "import math\n",
        "from sklearn.metrics import confusion_matrix,accuracy_score,classification_report\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_wjcRfFigJba"
      },
      "source": [
        "**Data Preprocessing** : Converting into lower case, applying lemmatization, removing stopwords and punctuation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "jt4SLj5_GNPY"
      },
      "outputs": [],
      "source": [
        "data=pd.read_excel('/content/drive/MyDrive/Annotated_Article_Dataset.xlsx')\n",
        "data=data.dropna()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "pdYoUWbv8Vfr"
      },
      "outputs": [],
      "source": [
        "#Preprocessing the Dataset\n",
        "def Preprocessing(text):\n",
        "\n",
        "  #converting into lower case\n",
        "  text=text.lower()\n",
        "\n",
        "  #lemmatization\n",
        "  lemmatizer = WordNetLemmatizer()\n",
        "  tokens = nltk.word_tokenize(text)\n",
        "  new_tokens=[] \n",
        "  #applying lemmatization\n",
        "  for token in tokens:\n",
        "    new_tokens.append(lemmatizer.lemmatize(token))\n",
        "\n",
        "  #Removing stopwords and Punctuation\n",
        "  stop = stopwords.words('english') + list(string.punctuation) + [\"''\",\"``\",\"..\"]\n",
        "  preprocessed = \" \".join(i for i in new_tokens if i not in stop)\n",
        "  return preprocessed\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "QKQF5c3S9L0T"
      },
      "outputs": [],
      "source": [
        "def preprocess(raw_data):\n",
        "  #articles\n",
        "  articles=[]\n",
        "  #label assigned to them\n",
        "  labels=[]\n",
        "  #Iterating through articles & label\n",
        "  for article,label in zip(raw_data.Article.iloc,raw_data.Annotation.iloc):    \n",
        "    article=Preprocessing(article)\n",
        "    articles.append(article)  \n",
        "    labels.append(label.upper())\n",
        "  Tuples = list(zip(articles, labels))  \n",
        "  return pd.DataFrame(Tuples, columns = ['Article', 'label']) \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iFps-q_m-W6c"
      },
      "source": [
        "**Preprocessed Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "i0FGBvNs99qM"
      },
      "outputs": [],
      "source": [
        "df=preprocess(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gy8WDTCBg3uT"
      },
      "source": [
        "**Plotting the data labels** : BJP, Congress, AAP and None (Not biased)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 355
        },
        "id": "iru7EG5FCczB",
        "outputId": "2542f7f8-9380-40b3-c50d-ab3418d8fbd0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Counter({'NONE': 334, 'BJP': 287, 'CONGRESS': 191, 'AAP': 87})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/seaborn/_decorators.py:43: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n",
            "  FutureWarning\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "import seaborn as sns\n",
        "def plot_count(Y_data):\n",
        "  y=Counter(Y_data)\n",
        "  print(y)\n",
        "  sns.countplot(Y_data)\n",
        "\n",
        "plot_count(df['label'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0oNRwCwqhLIO"
      },
      "source": [
        "**Splitting The DataSet**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "03UTitc-EktL"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score,classification_report,accuracy_score\n",
        "\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(df['Article'],df['label'], test_size=0.25,random_state=42,shuffle=True,stratify=df['label'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qR8jr6b0OD00"
      },
      "source": [
        "**BASELINE MODELS:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xlWPd6rRNjCd"
      },
      "source": [
        "**1. Naive Bayes without feature selection:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pGWykxfFNiK_",
        "outputId": "433314f6-505e-4224-c15b-42fcd4784048"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F1 Score:\n",
            "0.586546921450363\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         AAP       0.00      0.00      0.00        22\n",
            "         BJP       0.61      0.88      0.72        72\n",
            "    CONGRESS       1.00      0.27      0.43        48\n",
            "        NONE       0.64      0.83      0.72        83\n",
            "\n",
            "    accuracy                           0.64       225\n",
            "   macro avg       0.56      0.49      0.47       225\n",
            "weighted avg       0.64      0.64      0.59       225\n",
            "\n",
            "Accuracy:\n",
            "64.44444444444444\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "#Count Vectorizer\n",
        "Count=CountVectorizer()\n",
        "x_train=Count.fit_transform(X_train)\n",
        "x_test=Count.transform(X_test)\n",
        "\n",
        "#Tf-idf Transformer\n",
        "tf_idf=TfidfTransformer()\n",
        "x_train=tf_idf.fit_transform(x_train)\n",
        "x_test=tf_idf.transform(x_test)\n",
        "\n",
        "#Multinomial Naive Bayes\n",
        "clf = MultinomialNB().fit(x_train, Y_train)\n",
        "y_predicted = clf.predict(x_test)\n",
        "print(\"F1 Score:\")\n",
        "print(f1_score(Y_test, y_predicted,average='weighted'))\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(Y_test,y_predicted))\n",
        "print(\"Accuracy:\")\n",
        "print(accuracy_score(Y_test,y_predicted)*100)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "filename = '/content/drive/MyDrive/IR PROJECT/NB_Without_FeatureSelection.sav'\n",
        "pickle.dump(clf, open(filename, 'wb'))"
      ],
      "metadata": {
        "id": "CNTxcVfYzJxq"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-7uhkZcThTCL"
      },
      "source": [
        "**Feature Selection**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "EUzacpqND4ZX"
      },
      "outputs": [],
      "source": [
        "class_label={'BJP':0,'CONGRESS':1,'AAP':2,'NONE':3}\n",
        "reverse_class_label={0:'BJP',1:'CONGRESS',2:'AAP',3:'NONE'} \n",
        "def table_creation(X_train,Y_train):\n",
        "  #To keep the track of the Article Count of a term in a particular class\n",
        "  # table dictionary will have class count for each term.\n",
        "  table={}  \n",
        "  #Term count in a class           \n",
        "  term_count={}\n",
        "  #Number of Articles in a Class         \n",
        "  docs_count=np.array([0,0,0,0])\n",
        "  #Number of Words in a Class\n",
        "  words_count=np.array([0,0,0,0])\n",
        "  class_count=np.array([0,0,0,0])\n",
        "\n",
        "  for article,label in zip(X_train,Y_train):\n",
        "    docs_count[class_label[label]]+=1\n",
        "    words_count[class_label[label]]+=len(article)\n",
        "    unique_Tokens={-1}\n",
        "    for term in article.split():\n",
        "      #term count in of each term in each class\n",
        "      if term not in term_count:\n",
        "        term_count[term]=class_count.copy()\n",
        "      term_count[term][class_label[label]]+=1\n",
        "      unique_Tokens.add(term)\n",
        "    \n",
        "    #print(\"unique tokens in article are\",unique_Tokens)\n",
        "    unique_Tokens.remove(-1) \n",
        "\n",
        "    for term in unique_Tokens:\n",
        "      if term not in table:\n",
        "        table[term]=class_count.copy()\n",
        "      table[term][class_label[label]]+=1\n",
        "  \n",
        "  return table, words_count, docs_count, term_count\n",
        "  \n",
        "table, words_count, docs_count, term_count=table_creation(X_train,Y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LjtG9l_yhf0F"
      },
      "source": [
        "**Calculate Mutual Information**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dFIFtpG9KLDZ",
        "outputId": "589be64d-5caf-407e-e840-98ee31711cd6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total number of articles are 674\n"
          ]
        }
      ],
      "source": [
        "#Calculating Mutual Information for each term in each Class: BJP, AAP, Congress, None(NOT BIASED)\n",
        "def calculating_Mutual_Info(table,words_count,docs_count):\n",
        "  N=0\n",
        "  for i in docs_count:\n",
        "    N+=i\n",
        "  print(\"total number of articles are\",N)\n",
        "  #List to contain the mutual info for each class\n",
        "  mutual_Info_table=[[],[],[],[]] \n",
        "  for term in table:\n",
        "      for class_ in range(0,4):\n",
        "          #Term Present in class\n",
        "          N_11=table[term][class_]  \n",
        "          #Term Present but not in Class\n",
        "          N_10=np.sum(table[term])-N_11 \n",
        "          #Num of Docs in class NOT having term       \n",
        "          N_01=docs_count[class_]-N_11\n",
        "          #Num of Docs neither Term Nor Class\n",
        "          N_00=N-(N_01+N_10+N_11)                \n",
        "          \n",
        "          if N_11==0:\n",
        "              X=0\n",
        "          else:\n",
        "              X=(N_11/N) * ((np.log(N)+np.log(N_11)) - (np.log(N_11+N_01) + np.log(N_11+N_10)))\n",
        "          if N_01==0:\n",
        "              Y=0\n",
        "          else:\n",
        "              Y=(N_01/N) * ((np.log(N)+np.log(N_01)) - (np.log(N_01+N_00) + np.log(N_01+N_11)))\n",
        "          if N_10==0:\n",
        "              Z=0\n",
        "          else:\n",
        "              Z=(N_10/N) * ((np.log(N)+np.log(N_10)) - (np.log(N_10+N_11) + np.log(N_10+N_00)))\n",
        "          if N_00==0:\n",
        "              W=0\n",
        "          else:\n",
        "              W=(N_00/N) * ((np.log(N)+np.log(N_00)) - (np.log(N_00+N_01) + np.log(N_00+N_10)))\n",
        "          m=X+Y+Z+W\n",
        "          mutual_Info_table[class_].append(m)\n",
        "  return mutual_Info_table\n",
        "mutual_Info_table=calculating_Mutual_Info(table,words_count, docs_count)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "XXh9LU0lCBGM"
      },
      "outputs": [],
      "source": [
        "#creating word map, each word is assigned a uniuqe id like we did for each document\n",
        "def creating_map_of_words(list_of_words):\n",
        "    forward_map={}\n",
        "    reverse_map={}\n",
        "    count=0\n",
        "    for word in list_of_words:\n",
        "        forward_map[word] = count\n",
        "        count = count + 1\n",
        "    reverse_map = {v: k for k, v in forward_map.items()}\n",
        "    return forward_map,reverse_map\n",
        "forward_map,reverse_map=creating_map_of_words(table.keys())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "n4TUIeXWCDSF"
      },
      "outputs": [],
      "source": [
        "#this method will select feaures for each class and returns new vocabulary\n",
        "def feature_selection(mutual_Info_table,k,forward_map,reverse_map):\n",
        "    top_k_words=[]\n",
        "    for id in range(0,4):\n",
        "        temp = np.argsort(np.array(mutual_Info_table[id]))\n",
        "        temp = temp[::-1]\n",
        "        top_k_words.append(temp[:k].copy())\n",
        "    \n",
        "    new_vocab={-1}\n",
        "    count=0\n",
        "    for list_of_words in top_k_words:\n",
        "        for wordid in list_of_words:\n",
        "            new_vocab.add(reverse_map[wordid])\n",
        "        count=count+1\n",
        "        \n",
        "    new_vocab.remove(-1)\n",
        "    return top_k_words,new_vocab\n",
        "\n",
        "top_k_words,new_vocab=feature_selection(mutual_Info_table,200,forward_map,reverse_map)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "wHq3iRhICDhV"
      },
      "outputs": [],
      "source": [
        "#to calculate term and class probability \n",
        "def calc_prob(new_vocab,term_count,class_word_count,docs_count):\n",
        "    tprobability={}\n",
        "    beta=len(new_vocab)\n",
        "    for word in new_vocab:\n",
        "      tprobability[word]=[]\n",
        "      for id in range(0,4):\n",
        "        tot=class_word_count[id]\n",
        "        tc=term_count[word][id]\n",
        "        p=(tc+1)/(tot+beta)\n",
        "        tprobability[word].append(p)    \n",
        "    cprobability=[]\n",
        "    N=np.sum(docs_count)\n",
        "    for doc_count in docs_count:\n",
        "        cprobability.append(doc_count/N)\n",
        "    return cprobability,tprobability\n",
        "cprobability,tprobability=calc_prob(new_vocab,term_count,words_count,docs_count)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Ttev2E4iMQz"
      },
      "source": [
        "**2. Naive Bayes With Feature Selection:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "CmsV0uisexK4"
      },
      "outputs": [],
      "source": [
        "def naive_bayes(new_vocab,term_probability,class_probability,X_test):\n",
        "    y_predicted=[]\n",
        "    for article in X_test:\n",
        "        max_score=-math.inf\n",
        "        score=0\n",
        "        result_class=-1                          \n",
        "        for id in range(0,4):\n",
        "            score=np.log(class_probability[id])\n",
        "            for word in article.split():\n",
        "                if word in new_vocab:\n",
        "                    score=score+np.log(term_probability[word][id])\n",
        "            if score>max_score:\n",
        "                max_score=score\n",
        "                result_class=id\n",
        "        y_predicted.append(reverse_class_label[result_class])       \n",
        "    return y_predicted          \n",
        "\n",
        "y_predicted=naive_bayes(new_vocab,tprobability,cprobability,X_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ng8ThzdrC9Lu",
        "outputId": "316d161e-5c7a-4222-d5dd-28ef1d7c3888"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion Matrix :\n",
            "[[22  0  0  0]\n",
            " [ 3 58 10  1]\n",
            " [ 1  7 40  0]\n",
            " [11  9  5 58]]\n",
            "Accuracy Score : 79.11111111111111\n",
            "Classification Report : \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         AAP       0.59      1.00      0.75        22\n",
            "         BJP       0.78      0.81      0.79        72\n",
            "    CONGRESS       0.73      0.83      0.78        48\n",
            "        NONE       0.98      0.70      0.82        83\n",
            "\n",
            "    accuracy                           0.79       225\n",
            "   macro avg       0.77      0.83      0.78       225\n",
            "weighted avg       0.83      0.79      0.79       225\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print('Confusion Matrix :')\n",
        "print(confusion_matrix(Y_test, y_predicted)) \n",
        "print('Accuracy Score :',accuracy_score(Y_test, y_predicted)*100 )\n",
        "print('Classification Report : ')\n",
        "print(classification_report(Y_test, y_predicted) )"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# filename = '/content/drive/MyDrive/IR PROJECT/NB_With_FeatureSelection.sav'\n",
        "# pickle.dump(clf, open(filename, 'wb'))"
      ],
      "metadata": {
        "id": "bdBInPs-zpmo"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7EGq1C_5QK4x"
      },
      "source": [
        "**SVM without feature selection:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I7BqNNasDmQ8"
      },
      "source": [
        "**3. For Unigrams**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_PrZppK50gcg",
        "outputId": "d2b429e8-646b-4a81-c999-5e6531ad5eec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LibSVM]F1 Score:\n",
            "0.7581906958030066\n",
            "Classificatin Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         AAP       1.00      0.77      0.87        22\n",
            "         BJP       0.69      0.86      0.77        72\n",
            "    CONGRESS       0.74      0.48      0.58        48\n",
            "        NONE       0.80      0.84      0.82        83\n",
            "\n",
            "    accuracy                           0.76       225\n",
            "   macro avg       0.81      0.74      0.76       225\n",
            "weighted avg       0.77      0.76      0.76       225\n",
            "\n",
            "Accuracy:\n",
            "76.44444444444444\n"
          ]
        }
      ],
      "source": [
        "# SVM without feature Selection using different approaches:\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.svm import SVC \n",
        "\n",
        "#tf idf vectorizer\n",
        "vector=TfidfVectorizer(analyzer='word',ngram_range=(1,1))\n",
        "\n",
        "x_train=vector.fit_transform(X_train)\n",
        "x_test=vector.transform(X_test)\n",
        "\n",
        "clf=SVC(verbose=True)\n",
        "clf.fit(x_train,Y_train)\n",
        "y_predicted=clf.predict(x_test)\n",
        "print(\"F1 Score:\")\n",
        "print(f1_score(Y_test,y_predicted,average='weighted'))\n",
        "print(\"Classificatin Report:\")\n",
        "print(classification_report(Y_test,y_predicted))\n",
        "print(\"Accuracy:\")\n",
        "print(accuracy_score(Y_test,y_predicted)*100)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "filename = '/content/drive/MyDrive/IR PROJECT/SVM_Unigrams.sav'\n",
        "pickle.dump(clf, open(filename, 'wb'))"
      ],
      "metadata": {
        "id": "zq6BaWmxzujG"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O7nfUp52D1qA"
      },
      "source": [
        "**4. For Bigrams**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C0MK5cqADasF",
        "outputId": "47bb6b31-69b4-4540-b24d-c95723ca0510"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LibSVM]F1 Score:\n",
            "0.40810643609627506\n",
            "Classificatin Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         AAP       1.00      0.05      0.09        22\n",
            "         BJP       0.73      0.38      0.50        72\n",
            "    CONGRESS       0.50      0.04      0.08        48\n",
            "        NONE       0.44      0.98      0.61        83\n",
            "\n",
            "    accuracy                           0.49       225\n",
            "   macro avg       0.67      0.36      0.32       225\n",
            "weighted avg       0.60      0.49      0.41       225\n",
            "\n",
            "Accuracy:\n",
            "49.333333333333336\n"
          ]
        }
      ],
      "source": [
        "vector=TfidfVectorizer(analyzer='word',ngram_range=(2,2))\n",
        "x_train=vector.fit_transform(X_train)\n",
        "x_test=vector.transform(X_test)\n",
        "clf=SVC(verbose=True)\n",
        "clf.fit(x_train,Y_train)\n",
        "y_predicted=clf.predict(x_test)\n",
        "print(\"F1 Score:\")\n",
        "print(f1_score(Y_test,y_predicted,average='weighted'))\n",
        "print(\"Classificatin Report:\")\n",
        "print(classification_report(Y_test,y_predicted))\n",
        "print(\"Accuracy:\")\n",
        "print(accuracy_score(Y_test,y_predicted)*100)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "filename = '/content/drive/MyDrive/IR PROJECT/SVM_Bigrams.sav'\n",
        "pickle.dump(clf, open(filename, 'wb'))"
      ],
      "metadata": {
        "id": "miz2Wd7cz1j9"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ElUg5MTUi3n9"
      },
      "source": [
        "**5. SVM with Feature Selection**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "Zu4f71Sg4rZC"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "def count_vector(X_data,vocab,term_count):\n",
        "  data_count=[]\n",
        "  for index in X_data:\n",
        "    count_vector=[]\n",
        "    values=index.split(' ')\n",
        "    for word in vocab:\n",
        "      if word in values:\n",
        "        count_vector.append(values.count(word))\n",
        "      else:\n",
        "        count_vector.append(0)\n",
        "    data_count.append(count_vector)\n",
        "  return data_count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "YuDXiSaNRm-x"
      },
      "outputs": [],
      "source": [
        "x_train=count_vector(X_train,new_vocab,term_count)\n",
        "x_test=count_vector(X_test,new_vocab,term_count)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "0aELIktzD-0i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "224b6db8-1f11-4359-e46b-8f5fd7284d80"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LibSVM]F1 Score:\n",
            "0.8626810975235463\n",
            "Classificatin Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         AAP       0.96      1.00      0.98        22\n",
            "         BJP       0.76      0.90      0.82        72\n",
            "    CONGRESS       0.88      0.73      0.80        48\n",
            "        NONE       0.95      0.87      0.91        83\n",
            "\n",
            "    accuracy                           0.86       225\n",
            "   macro avg       0.88      0.87      0.88       225\n",
            "weighted avg       0.87      0.86      0.86       225\n",
            "\n",
            "Accuracy:\n",
            "86.22222222222223\n"
          ]
        }
      ],
      "source": [
        "vector=TfidfTransformer()\n",
        "x_train=vector.fit_transform(x_train)\n",
        "x_test=vector.transform(x_test)\n",
        "clf=SVC(verbose=True)\n",
        "clf.fit(x_train,Y_train)\n",
        "y_predicted=clf.predict(x_test)\n",
        "print(\"F1 Score:\")\n",
        "print(f1_score(Y_test,y_predicted,average='weighted'))\n",
        "print(\"Classificatin Report:\")\n",
        "print(classification_report(Y_test,y_predicted))\n",
        "print(\"Accuracy:\")\n",
        "print(accuracy_score(Y_test,y_predicted)*100)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "filename = '/content/drive/MyDrive/IR PROJECT/SVM_With_FeatureSelection.sav'\n",
        "pickle.dump(clf, open(filename, 'wb'))"
      ],
      "metadata": {
        "id": "61NQ68_Uz4yZ"
      },
      "execution_count": 26,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Mid_Project_IR(final).ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}